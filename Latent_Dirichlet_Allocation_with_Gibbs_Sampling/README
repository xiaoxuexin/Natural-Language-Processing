For this homework, you’ll complete an implementation of latent Dirichlet allocation (LDA) and run it on real data, discovering clusters of documents in Wikipedia. As a part of the homework, you’ll finishing the part of the algorithm that learns the word-topic assignments. While there are many ways to implementLDA,here,we’llbeusingGibbsSampling.2 ThemathbehindGibbssamplingcanbe quite complex but there is a good introduction to it for NLP folks with no background on the topic3 and the first chapter of the textbook by Boyd-Graber, Hu, and Mimno in the course readings has a great high-level summary of the algorithm. As with many of the techniques we’ve encountered, your job will boil down to counting things and there’s several good blog posts out there to help you understand how the math connections to a very simple implementation.

While there’s not much programming you have to do for this assignment (probably less than 30 lines of code), it depends on understanding the rest of the code around it. Don’t leave it until the last minute. The main goals for this assignment are (1) to help you understand how the mathematical descriptions of LDA connect to how it’s actually implemented and (2) to have you implement a simple but core part of the algorithm that’s responsible for its learning. Most of the effort for this assignment will be spent thinking to develop your understanding, rather than writing lots of code, so if you find yourself spending a few hours working through the code and reading without writing a single line, that’s expected. Once you understand what you have to do, the pieces will fall into place quickly and the scaffolding code we’re providing is intended to help get you to that understanding. As you get started, we recommend trying to trace the algorithmic description of LDA through the code and match each part, which will help you understand how the parts your implementing connect with the whole algorithm.
